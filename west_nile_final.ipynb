{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ West Nile Project 4 $\n",
    "### $ Predicting West Nile in Chicago $\n",
    "#### Contributors: Will Suh, Ahbishek Sharma, Uday Datta, Jon Lau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "1tNhlKV5U9_1",
    "outputId": "c9303615-34f6-46f3-bc91-fb6a818c7499",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install vincenty #measures distance between lat and long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "4XHnGGLbP2-C",
    "outputId": "e2e0fd0c-90ce-4d7b-e214-9235e5fe2f57"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from vincenty import vincenty \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "%autocall 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MNJID_a4EQE"
   },
   "source": [
    "# Importing Datasets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "tgMgK5edRfx8",
    "outputId": "2f7dbf3f-1c75-41b2-e0c6-dee9fcd83f5a"
   },
   "outputs": [],
   "source": [
    "spray = pd.read_csv(\"./data/west_nile/input/spray.csv\")\n",
    "spray.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "U1dsySFPQdzR",
    "outputId": "8c80624e-75c0-4d71-ccec-e499a9fe35d1"
   },
   "outputs": [],
   "source": [
    "#weather_file = files.upload()\n",
    "weather = pd.read_csv('./data/west_nile/input/weather.csv')\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "Murm8hGeQe64",
    "outputId": "e83341c1-fd29-47d3-a796-c44c296aaa61",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_file = files.upload()\n",
    "train = pd.read_csv('./data/west_nile/input/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "DiiUfa3E5RIs",
    "outputId": "506fad74-a3f8-49eb-b3b9-a1b4b2e4b41c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import Test File\n",
    "#test_file = files.upload()\n",
    "test = pd.read_csv('./data/west_nile/input/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXa-D5D95T6H"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4iFGS506O6x"
   },
   "outputs": [],
   "source": [
    "def EDA(df):\n",
    "    null_vals = df.isnull().sum()[df.isnull().sum() > 0] \n",
    "    shape = df.shape\n",
    "    dtypes = df.dtypes\n",
    "    print('Nulls:', null_vals)\n",
    "    print('Shape:', shape)\n",
    "    print('Data Types:', dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1343
    },
    "colab_type": "code",
    "id": "Z_NNHqso6Qgc",
    "outputId": "f8ef647d-4a24-47ae-e07f-c70bba331f15"
   },
   "outputs": [],
   "source": [
    "print(EDA(train))\n",
    "print(EDA(test))\n",
    "print(EDA(spray))\n",
    "print(EDA(weather))\n",
    "#print(EDA(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " No np.NaNs in our data. Nulls are represented with M in some fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date is an object an int\n",
    "def convert_date(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_date(train)\n",
    "convert_date(test)\n",
    "convert_date(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "vQqhBnV0C5RM"
   },
   "outputs": [],
   "source": [
    "#plotted mosquitos by trap by date\n",
    "train[['Date', 'Trap', 'NumMosquitos']].groupby(by = ['Date','Trap'])['Date','Trap','NumMosquitos'] \\\n",
    "    .sum().reset_index().sort_values('NumMosquitos', ascending = False).set_index('Date').plot(style = '.')\n",
    "    \n",
    "plt.title('Mosquitos by Trap over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mosquitos at Traps');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotted WNV incidents by trap\n",
    "train[['Date', 'Trap','WnvPresent']].groupby(by = ['Date','Trap'])['Date','Trap','WnvPresent'] \\\n",
    "    .sum().reset_index().sort_values('WnvPresent', ascending = False).set_index('Date').plot(style = '.')\n",
    "    \n",
    "plt.title('Positive WnV Cases by Trap over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('WnV Present at Traps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowing us to visually undestand out timeseries data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eb8rZ5Vr1Tk-"
   },
   "outputs": [],
   "source": [
    "#Create new column for combined Lat and Long\n",
    "train['LatLong'] = list(zip(train.Latitude, train.Longitude))\n",
    "test['LatLong'] = list(zip(test['Latitude'], test['Longitude']))\n",
    "\n",
    "station1 = (41.995, -87.933)\n",
    "station2 = (41.786, -87.752)\n",
    "train['Closest_Station'] = [ 1 if vincenty(x,station1) < vincenty(x,station2) else 2 for x in train['LatLong']]\n",
    "test['Closest_Station'] = [ 1 if vincenty(x,station1) < vincenty(x,station2) else 2 for x in test['LatLong']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZDBttWb1_rY"
   },
   "outputs": [],
   "source": [
    "#Merge DataFramne\n",
    "\n",
    "train_weather = train.merge(weather,how = 'left', left_on = ['Date','Closest_Station'],right_on =['Date','Station'])\n",
    "test_weather = test.merge(weather,how = 'left', left_on = ['Date','Closest_Station'],right_on =['Date','Station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdgR_HRHeaEk"
   },
   "outputs": [],
   "source": [
    "train_weather = train_weather.drop(columns = ['SeaLevel','CodeSum', 'Sunrise', 'Sunset','Depart','Depth','Water1', 'SnowFall', 'Cool', 'Heat','StnPressure', 'AvgSpeed','ResultSpeed','ResultDir','NumMosquitos'])\n",
    "test_weather =test_weather.drop(columns = ['SeaLevel','CodeSum', 'Sunrise', 'Sunset','Depart','Depth','Water1', 'SnowFall', 'Cool', 'Heat','StnPressure', 'AvgSpeed','ResultSpeed','ResultDir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns that we don't se as useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnk5f_vw2PFm"
   },
   "outputs": [],
   "source": [
    "#filling in missing(M) and trace(T)\n",
    "weather_dataset = train_weather.columns.tolist()\n",
    "\n",
    "for col in weather_dataset:\n",
    "    for row in range(train_weather.shape[0]):\n",
    "        if train_weather.loc[row, col] == 'M' or train_weather.loc[row, col] == '  T':\n",
    "            train_weather.loc[row, col] = train_weather.loc[row - 1, col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6AB4BZaJJU8"
   },
   "outputs": [],
   "source": [
    "weather_dataset2 = test_weather.columns.tolist()\n",
    "\n",
    "for col in weather_dataset2:\n",
    "    for row in range(test_weather.shape[0]):\n",
    "        if test_weather.loc[row, col] == 'M' or test_weather.loc[row, col] == '  T':\n",
    "            test_weather.loc[row, col] = test_weather.loc[row - 1, col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fills in the T and M data with closest data. Time series allows us to do this knowing that the data is corrlated. \n",
    "This allows the data to be all numerical for modeling. (same for cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_object_dtypes = ['Tavg', 'WetBulb', 'PrecipTotal']\n",
    "\n",
    "for col in weather_object_dtypes:\n",
    "    train_weather[col] = pd.to_numeric(train_weather[col])\n",
    "    test_weather[col] = pd.to_numeric(test_weather[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosquito(df):\n",
    "    #split wnv transmitting mosquito species lines into separate columns\n",
    "    df['CULEX PIPIENS'] = 0\n",
    "    df['CULEX RESTUANS'] = 0\n",
    "    \n",
    "    for row in range(df.shape[0]):\n",
    "        if df.loc[row, 'Species'] == 'CULEX PIPIENS/RESTUANS':\n",
    "            df.loc[row, 'CULEX PIPIENS'] == 1\n",
    "            df.loc[row, 'CULEX RESTUANS'] == 1\n",
    "        elif df.loc[row, 'Species'] == 'CULEX PIPIENS':\n",
    "            df.loc[row, 'CULEX PIPIENS'] == 1\n",
    "        elif df.loc[row, 'Species'] == 'CULEX RESTUANS':\n",
    "            df.loc[row, 'CULEX RESTUANS'] == 1\n",
    "            \n",
    "    \n",
    "    df.drop(columns = ['Species'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call function\n",
    "mosquito(train_weather)\n",
    "mosquito(test_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weather = pd.get_dummies(train_weather, columns = ['Block', 'Trap'])\n",
    "test_weather = pd.get_dummies(test_weather, columns = ['Block', 'Trap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the categorical data into numerical data with function and get dummeies to prepare for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NX7t3qGN-fwZ"
   },
   "outputs": [],
   "source": [
    "# interaction features\n",
    "train_weather['wet_temp'] = train_weather['PrecipTotal']*train_weather['Tavg']\n",
    "train_weather['wet_temp_roll'] = train_weather['wet_temp'].rolling(3).mean()\n",
    "train_weather['wet_temp_roll'].fillna(0, inplace = True)\n",
    "train_weather['wet_temp_roll_shift14'] = train_weather['wet_temp_roll'].shift(14)\n",
    "train_weather['wet_temp_roll_shift14'].fillna(0, inplace = True)\n",
    "train_weather['wet_temp_roll_shift7'] = train_weather['wet_temp_roll'].shift(7)\n",
    "train_weather['wet_temp_roll_shift7'].fillna(0, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cw09X84PIquo"
   },
   "outputs": [],
   "source": [
    "test_weather['wet_temp'] = test_weather['PrecipTotal']*test_weather['Tavg']\n",
    "test_weather['wet_temp_roll'] = test_weather['wet_temp'].rolling(3).mean()\n",
    "test_weather['wet_temp_roll'].fillna(0, inplace = True)\n",
    "test_weather['wet_temp_roll_shift14'] = test_weather['wet_temp_roll'].shift(14)\n",
    "test_weather['wet_temp_roll_shift14'].fillna(0, inplace = True)\n",
    "test_weather['wet_temp_roll_shift7'] = test_weather['wet_temp_roll'].shift(7)\n",
    "test_weather['wet_temp_roll_shift7'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating interactive feature of Temp and Precipation and then doing a rolling 3 day mean. Then also shifting that data 1 and 2 weeks. This gives us one feature to account for best mosquito temps and time shifts it for the hatching and larvea time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkF8SSx73-ov"
   },
   "outputs": [],
   "source": [
    "def date_split(df):\n",
    "    #breaking week, month, and year into separate columns\n",
    "    \n",
    "    df['Week'] = df['Date'].dt.week\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "\n",
    "    #drop date column\n",
    "    df.drop(columns = 'Date', inplace = True)\n",
    "\n",
    "    #get dummies\n",
    "    return pd.get_dummies(df, columns = ['Week'])\n",
    "    return pd.get_dummies(df, columns = ['Month'])\n",
    "    return pd.get_dummies(df, columns = ['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_07xOw814sk6"
   },
   "outputs": [],
   "source": [
    "#call function\n",
    "date_split(train_weather)\n",
    "date_split(test_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "BjtGB8-ntgmq",
    "outputId": "aa458a15-d92e-4939-8538-b121737f9f1b"
   },
   "outputs": [],
   "source": [
    "# Plotting out WNV Present by Species\n",
    "grouped = train.groupby([\"Species\"])\n",
    "grouped_percentage = pd.DataFrame()\n",
    "grouped_percentage[\"Number of Total Instances\"] = train[\"Species\"].value_counts()\n",
    "grouped_percentage[\"WnvNotPresent Rate\"] = grouped[\"WnvPresent\"].apply(lambda x : x.value_counts()[0]/len(x) )\n",
    "grouped_percentage[\"WnvPresent Rate\"] = 1 - grouped_percentage[\"WnvNotPresent Rate\"]\n",
    "grouped_percentage[\"WnvNotPresent Instances\"] = grouped_percentage[\"Number of Total Instances\"] * grouped_percentage[\"WnvNotPresent Rate\"]\n",
    "grouped_percentage[\"WnvPresent Instances\"] = grouped_percentage[\"Number of Total Instances\"] - grouped_percentage[\"WnvNotPresent Instances\"]\n",
    "\n",
    "#grouped_percentage\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.bar(grouped_percentage.index ,grouped_percentage[\"WnvPresent Rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weather.drop(columns = ['Address','Street', 'AddressNumberAndStreet', 'Latitude', \\\n",
    "                                              'Longitude', 'AddressAccuracy','LatLong'], inplace = True)\n",
    "test_weather.drop(columns = ['Address','Street', 'AddressNumberAndStreet', 'Latitude', \\\n",
    "                                              'Longitude', 'AddressAccuracy','LatLong'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing some columns that are not needed for modeling. Block get dummies covers a lot of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3w0GPPpMuCZz"
   },
   "outputs": [],
   "source": [
    "# Train-Train-Split on Data Set\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "X = train_weather.drop('WnvPresent', axis =1)\n",
    "y = train_weather['WnvPresent']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = X_train.merge(pd.DataFrame(y_train), how = 'left', right_index = True, left_index = True)\n",
    "train_majority = traindata[traindata['WnvPresent'] == 0]\n",
    "train_minority = traindata[traindata['WnvPresent'] == 1]\n",
    "train_minority_upsampled = resample(train_minority, \n",
    "                                     replace = True, \n",
    "                                     n_samples = train_majority.shape[0],\n",
    "                                     random_state = 42)\n",
    "\n",
    "train_data_upsampled = pd.concat([train_majority, train_minority_upsampled])\n",
    "X_train = train_data_upsampled.drop(columns = 'WnvPresent')\n",
    "y_train = train_data_upsampled['WnvPresent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMzjeUIcDHg_"
   },
   "outputs": [],
   "source": [
    "def drop_columns(df1, df2):\n",
    "    #drop columns in either test/train that are not in the other\n",
    "    \n",
    "    df1cols = df1.columns.tolist()\n",
    "    df2cols = df2.columns.tolist()\n",
    "    \n",
    "    notindf1cols = []\n",
    "    notindf2cols = []\n",
    "    \n",
    "    for col in df1cols:\n",
    "        if col not in df2cols:\n",
    "            notindf2cols.append(col)\n",
    "    \n",
    "    for col in df2cols:\n",
    "        if col not in df1cols:\n",
    "            notindf1cols.append(col)\n",
    "            \n",
    "    df1.drop(columns = notindf2cols, inplace = True)\n",
    "    df2.drop(columns = notindf1cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLQq0U5hDPuh"
   },
   "outputs": [],
   "source": [
    "drop_columns(test_weather, train_weather)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure our Train and Test data is aligned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking Class Balance\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "FNhuRpsDuCVp",
    "outputId": "66f0d591-72e5-40c1-81af-f3008ba1c44d"
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# rf_pipe = Pipeline([\n",
    "#     ('ss', ss),\n",
    "#     ('rf', rf)\n",
    "# ])\n",
    "\n",
    "# params = {'rf__n_estimators' : [150,200,250,300],\n",
    "#           'rf__max_depth' : [None, 2, 3, 4, 5]}\n",
    "\n",
    "# rf_gs = GridSearchCV(rf_pipe, param_grid=params, cv=5, scoring='roc_auc',n_jobs= 5)\n",
    "# rf_gs.fit(X_train, y_train)\n",
    "\n",
    "# best_rf_gs = rf_gs.best_estimator_\n",
    "\n",
    "# rf_gs_train = best_rf_gs.score(X_train, y_train)\n",
    "# rf_gs_test = best_rf_gs.score(X_test, y_test)\n",
    "\n",
    "# print(best_rf_gs)\n",
    "# print(rf_gs_train)\n",
    "# print(rf_gs_test)\n",
    "# print(rf_gsbest_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used above GS to determine best params to run on Random Forest below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators= 100, n_jobs = 5)\n",
    "rf.fit(X_train,y_train)\n",
    "rf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "\n",
    "test_names = X.columns.tolist()\n",
    "rf_importances = pd.DataFrame(sorted(zip(test_names, rf.feature_importances_), reverse = True), columns = ['Variable', 'Importance']).set_index('Variable')\n",
    "rf_importances.sort_values(by = 'Importance', ascending = False).iloc[:20,:].plot(kind = 'bar')\n",
    "plt.title('Random Forest Feature Importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "y_pred_proba = rf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(fpr,tpr,lw = 3, color='blue', label='ROC Curve %.5f' % auc)\n",
    "plt.plot([0,1], [0,1], lw = 3, linestyle ='--', color='grey')\n",
    "plt.legend(loc=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Random Forest ROC Curve')\n",
    "plt.savefig('gbc_roc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Create a heatmap confusion matrix - get predictions \n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Create confusion matrix \n",
    "classes = [\"No WNV\", 'WNV']\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm = pd.DataFrame(cm, columns=classes)\n",
    "cm.index = classes\n",
    "\n",
    "# Plot matrix on heatmap \n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('RF Model');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADABoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adamodel = AdaBoostClassifier(n_estimators=100) \n",
    "ada_scores = cross_val_score(adamodel, X_train, y_train, cv=5)\n",
    "adamodel.fit(X_train,y_train)\n",
    "adamodel.score(X_train,y_train)\n",
    "y_preds = adamodel.predict(X_test)\n",
    "adamodel.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_importances = pd.DataFrame(sorted(zip(test_names, adamodel.feature_importances_), reverse = True), columns = ['Variable', 'Importance']).set_index('Variable')\n",
    "ada_importances.sort_values(by = 'Importance', ascending = False).iloc[:20,:].plot(kind = 'bar')\n",
    "plt.title('Ada Boost Feature Importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapredicts = pd.DataFrame(y_preds, columns = ['predict'])\n",
    "#adapredicts.columns\n",
    "adapredicts['predict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = adamodel.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(fpr,tpr,lw = 3, color='blue', label='ROC Curve %.5f' % auc)\n",
    "plt.plot([0,1], [0,1], lw = 3, linestyle ='--', color='grey')\n",
    "plt.legend(loc=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Ada Boost ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Create a heatmap confusion matrix - get predictions \n",
    "predictions = adamodel.predict(X_test)\n",
    "\n",
    "# Create confusion matrix \n",
    "classes = [\"No WNV\", 'WNV']\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm = pd.DataFrame(cm, columns=classes)\n",
    "cm.index = classes\n",
    "\n",
    "# Plot matrix on heatmap \n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('ADA Boost');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "Bjws86bduCS9",
    "outputId": "83ebc0b7-9951-4ef5-e3b4-5ecadf23353d"
   },
   "outputs": [],
   "source": [
    "# XGBoost Classifier (This takes about 5 min to finish)\n",
    "\n",
    "gs_params = {\n",
    "    'max_depth':[1, 2, 3, 4, 5],\n",
    "    'n_estimators':range(1, 10, 1),\n",
    "    'learning_rate':np.logspace(-5,0,5),\n",
    "    'silent' : [False],\n",
    "    'booster' : ['gbtree', 'gblinear', 'dart'] \n",
    "}\n",
    "\n",
    "xgb_gs = GridSearchCV(XGBClassifier(), gs_params, cv=5, verbose=1, scoring='roc_auc',n_jobs = -1)\n",
    "\n",
    "xgb_gs = xgb_gs.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_gs = xgb_gs.best_estimator_\n",
    "\n",
    "xgb_gs_train = best_xgb_gs.score(X_train, y_train)\n",
    "xgb_gs_test = best_xgb_gs.score(X_test, y_test)\n",
    "\n",
    "print(best_xgb_gs)\n",
    "print(xgb_gs_train)\n",
    "print(xgb_gs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EIHcvkos_UIa",
    "outputId": "1e221b92-2846-4144-d472-97176538509f"
   },
   "outputs": [],
   "source": [
    "# Gridsearch on logistic regression model above\n",
    "lr_params = {'penalty':['l1', 'l2'], \n",
    "             'C': np.logspace(-5, 2, 10)}\n",
    "gslr = GridSearchCV(LogisticRegression(), param_grid = lr_params)\n",
    "gslr.fit(X_train, y_train)\n",
    "\n",
    "# Results \n",
    "gslr.best_score_, gslr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gslr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrpredict = gslr.predict(X_test)\n",
    "roc_auc_score(y_test, lrpredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(gslr.best_estimator_.coef_[0], index = X.columns, columns = ['coef'])\n",
    "coefs.sort_values(by='coef', ascending = False, inplace=True)\n",
    "coefs.head(20).plot(kind = 'barh')\n",
    "plt.title('Logistic Regression Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap confusion matrix - get predictions \n",
    "predictions = gslr.predict(X_test)\n",
    "\n",
    "# Create confusion matrix \n",
    "classes = [\"No WNV\", 'WNV']\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm = pd.DataFrame(cm, columns=classes)\n",
    "cm.index = classes\n",
    "\n",
    "# Plot matrix on heatmap \n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Logistic Regression');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = gslr.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(fpr,tpr,lw = 3, color='blue', label='ROC Curve %.5f' % auc)\n",
    "plt.plot([0,1], [0,1], lw = 3, linestyle ='--', color='grey')\n",
    "plt.legend(loc=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Log Reg ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "D9tF9C8i7VLS",
    "outputId": "b514dde7-ff6d-4383-9d20-244ebb0679f0"
   },
   "outputs": [],
   "source": [
    "# Executive Summary of Models\n",
    "\n",
    "print('GridSearchCV across Random Forest:')\n",
    "print(f\"Best Parameters = {rf_gs.best_params_}\")\n",
    "print(f\"Best CV Score = {rf_gs.best_score_}\")\n",
    "print(f\"Train Score = {rf_gs_train}\")\n",
    "print(f\"Test Score = {rf_gs_test}\")\n",
    "print()\n",
    "print('GridSearchCV across XGBoost:')\n",
    "print(f\"Best Parameters = {xgb_gs.best_params_}\")\n",
    "print(f\"Best CV Score = {xgb_gs.best_score_}\")\n",
    "print(f\"Train Score = {xgb_gs_train}\")\n",
    "print(f\"Test Score = {xgb_gs_test}\")\n",
    "print()\n",
    "print('GridSearchCV across BalancedBaggingClassifier:')\n",
    "print(f\"Best Parameters = {bbc_gs.best_params_}\")\n",
    "print(f\"Best CV Score = {bbc_gs.best_score_}\")\n",
    "print(f\"Train Score = {bbc_train}\")\n",
    "print(f\"Test Score = {bbc_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We picked ADAboost as it optimized for False Negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = adamodel.predict(test_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrta3HPaQf8m"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test['WnvPresent'] = predict\n",
    "test[['Id','WnvPresent']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['Id','WnvPresent']].to_csv('submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "west_nile.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
